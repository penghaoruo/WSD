\documentclass[11pt,letterpaper]{article}
\oddsidemargin 0in
\evensidemargin 0in
\textwidth 6.5in
\topmargin -0.5in
\textheight 9.0in
\usepackage{mathptmx}
\usepackage{graphicx}
\usepackage{natbib} % for references
\usepackage[usenames,dvipsnames]{xcolor}

\begin{document}

\title{A Semi-supervised Approach to Word Sense Disambiguation \\ \small{CS446 Class Project}}
\author{Haoruo Peng(hpeng7@illinois.edu) \and Shyam Upadhyay(upadhya3@illinois.edu)}
\maketitle
\section*{Task description}
In natural language, a word may be associated with possibly multiple meanings, depending on the context in which the word occurs. For instance, the word pen has the following senses according to Wordnet:
\begin{enumerate}
\item \emph{pen : a writing implement with a point from which ink flows}
\item \emph{pen : an enclosure for confining livestock}
\end{enumerate}
Word sense disambiguation is the problem of determining the correct sense of a word in a given sentence. Word sense disambiguation can be viewed as a multi class classification problem, where each word admits several possible senses and the task is to identify the correct sense of a given word given its context. For example, determining the correct sense of the word ``pen" in the following passage:

\emph{Little John was looking for his toy box. Finally he found it. The box was in the pen. John was very happy.}

A naive baseline for word sense disambiguation will be to output the most frequent sense of the word In this project, we will investigate into the performance of semi-supervised learning algorithms for word sense disambiguation.

\section*{Background}
A general survey is provided in~\cite{navigli2009word}. WSD has been described as an AI-complete problem~\cite{mallery1988thinking}. Researchers have done much progress to WSD problem achieving sufficiently high levels of accuracy on a variety of word types and ambiguities. A rich variety of techniques have been researched, from dictionary-based methods~\cite{mihalcea2007using} that use the knowledge encoded in lexical resources, to supervised machine learning methods~\cite{manning1999foundations} in which a classifier is trained for each distinct word on a corpus of manually sense-annotated examples, to completely unsupervised methods~\cite{yarowsky1995unsupervised} that cluster occurrences of words, thereby inducing word senses.

Supervised learning approaches have been remarkably successful for performing word sense disambiguation. But the lack of sense-tagged data poses a severe bottleneck. To address this problem, researchers have resorted to semi-supervised learning algorithms. The Yarowsky algorithm~\cite{yarowsky1995unsupervised} was an early example of such an algorithm. It uses the `One sense per collocation' and the `One sense per discourse' properties of human languages for word sense disambiguation. From observation, words tend to exhibit only one sense in most given discourse and in a given collocation. 

~\cite{le2008semi}, which allows both labeled and unlabeled data. 
More recently, researchers have leveraged word alignment information from parallel corporas to aid in obtaining coarse grained senses. In word alignment tasks, a wor 
Unlike sense-tagged datasets, good quality parallel corpora are readily available. 

%The bootstrapping approach starts from a small amount of seed data for each word: either manually tagged training examples or a small number of surefire decision rules (e.g., `play' in the context of `bass' almost always indicates the musical instrument). The seeds are used to train an initial classifier, using any supervised method. This classifier is then used on the untagged portion of the corpus to extract a larger training set, in which only the most confident classifications are included. The process repeats, each new classifier being trained on a successively larger training corpus, until the whole corpus is consumed, or until a given maximum number of iterations is reached.

Other semi-supervised techniques use large quantities of untagged corpora to provide co-occurrence information that supplements the tagged corpora. These techniques have the potential to help in the adaptation of supervised models to different domains.


\section*{Data and evaluation}
For general tasks, we will use datasets from \textbf{SemEval} ``http://www.senseval.org". It is an ongoing series of evaluations of computational semantic analysis systems; it evolved from the Senseval word sense evaluation series. The evaluations are intended to explore the nature of meaning in language. While meaning is intuitive to humans, transferring those intuitions to computational analysis has proved elusive.

We will also test our developed system on the medical disambiguation data ``http://wsd.nlm.nih.gov". This test collection was constructed using a method that automatically extracts instances of ambiguous terms from \textbf{MEDLINE} without manual curation which also uses \textbf{MeSH} indexing of \textbf{MEDLINE} as a resource. The resulting data set contains both biomedical terms and abbreviations and is automatically created using the \textbf{UMLS Metathesaurus} and the manual \textbf{MeSH} indexing of \textbf{MEDLINE}. 

\section*{Your approach} 
Semi-supervised approach for word sense disambiguation was first used in Yarowsky (1995)~\cite{yarowsky1995unsupervised}. Our approach to the problem will be similar in that we will also use a small set of labelled examples for seeding our decision parameters. Once we have developed a prototype, we will make incremental improvements and try to achieve performance comparable to the state of the art.

\section*{Your to-do list}
\begin{enumerate}
\item We have medical disambiguation data
\item We have mentioned the related work in this proposal.
\item We will use the semi-supervised approach of Yarowsky. We plan to implement it ourselves. (by week 2)
\item We will compare the performance of our system with the state of the art by comparing the F1 scores on standard datasets.
\item Week 6 - Writeup and Conclusion.
\end{enumerate}

\bibliographystyle{plain}
\bibliography{wsd}
\end{document}
