\documentclass[11pt,letterpaper]{article}
\oddsidemargin 0in
\evensidemargin 0in
\textwidth 6.5in
\topmargin -0.5in
\textheight 9.0in
\usepackage{hyperref}
\usepackage{mathptmx}
\usepackage{graphicx}
\usepackage{natbib} % for references
\usepackage[usenames,dvipsnames]{xcolor}
\newcommand{\blue}[1]{\textcolor{RoyalBlue}{#1}}
\newcommand{\fillme}[1]{\blue{\texttt{[Insert #1]}}}
\newcommand{\instructions}[1]{\blue{\textit{#1}}}
% uncomment the next two lines if you want the instructions to disappear.
%\renewcommand{\instructions}[1]{}
%\renewcommand{\fillme}[1]{}

\begin{document}

\title{A Semi-supervised Approach to Word Sense Disambiguation \\ \Large{(CS446 Class Project)}}
\author{Haoruo Peng(hpeng7@illinois.edu) \and Shyam Upadhyay(upadhya3@illinois.edu)}
\maketitle



%\instructions{If you are taking CS446 for 4 hours credit, you need to
%  do a research project. This is a template for the intermediate
%  report (the check in),
%  but this should also give you a start on the final report.
%The template for the final report is at
%\url{http://courses.engr.illinois.edu/cs446/Projects/CS446projCheckIn.tex}
%(or
%\url{http://courses.engr.illinois.edu/cs446/Projects/CS446projCheckIn.pdf}
%for the pdf). Uncomment the \texttt{$\backslash$renewcommand{$\backslash$instructions}[1]\{\}}
% and \texttt{$\backslash$renewcommand{$\backslash$fillme}[1]\{\}} lines in the preamble of the template if
% you want the instructions to disappear.
%% \begin{verbatim}
%% 
%% \end{verbatim}
%}


\begin{abstract}
We investigate the task of word sense disambiguation using a semi-supervised approach. Word sense disambiguation is the task of identifying the correct sense for a word which can possibly admit multiple senses. We use the Semeval-2007 dataset to evaluate the performance of our system. We use a graph based approach to disambiguate senses; the underlying assumption is that the sense of the neighboring words can assist in inferring the sense of the target word. In this respect, the senses are learnt in a somewhat joint-manner. Using appropriate weights in the graph constructed for all possible senses, we can use the shortest path algorithm to obtain the correct senses.
%\instructions{Very briefly, summarize your task, your model and your main results}
\end{abstract}


\section{Introduction} 
\label{sec:introduction}
%\instructions{This should be a brief outline of the paper -- use plain English, no math. Note that you should be able to write most of this section before you actually perform any experiments. First, define and motivate your task: what are you trying to learn, and why is this an important task? Second, define what kind of a machine learning problem this requires you to solve (binary/multiclass classification, ranking, ....). What is an appropriate baseline model for this task? What kind of model are you proposing? 
%Briefly summarize the assumptions your model makes. Finally, describe the hypotheses you wish to test. These are typically statements of the form ``we expect model/features A to perform better on this task than model/features B''.
%Outline how your experiments will evaluate these hypotheses (comparisons of different models, ablation studies, learning curves, oracle experiments... ). }

In natural language, a word may be associated with possibly multiple meanings, depending on the context in which the word occurs. For instance, the word pen has the following senses according to Wordnet ~\cite{wordnet}:
\begin{itemize}
\item \emph{pen : a writing implement with a point from which ink flows}
\item \emph{pen : an enclosure for confining livestock}
\end{itemize}
Word sense disambiguation(WSD) is the problem of determining the correct sense of a word in a given sentence. For example, determining the correct sense of the word ``pen" in the following passage:
\emph{Little John was looking for his toy box. Finally he found it. The box was in the pen. John was very happy.}
WSD can be viewed as a multi class classification problem, where each word admits several possible senses and the task is to identify the correct sense of a given word given its context and knowledge sources. The inherent difficulty of WSD is evident from the fact that the target classes change for each word in the lexicon. In this respect, WSD involves training $n$ different classifiers, one for each word in a lexicon of size $n$. 
%\subsection{Motivation}
WSD is a key component for natural language processing systems which involves semantic interpretation of text. Tasks such as machine translation, information retrieval, data mining, web data analysis can greatly benefit from text disambiguation tools. 


\section{Background}
\label{sec:background}
%\instructions{Summarize and discuss related work that you are building on: this requires you to find, read and cite a few research papers. This is also something you can get started on as soon as you have settled on a task.} 

WSD has been described as an AI-complete problem~\cite{mallery1988thinking}. Researchers have done much progress to WSD problem achieving sufficiently high levels of accuracy on a variety of word types and ambiguities. A rich variety of techniques have been used, from dictionary-based methods~\cite{mihalcea2007using} that use the knowledge encoded in lexical resources, to supervised machine learning methods~\cite{manning1999foundations} in which a classifier is trained for each distinct word on a corpus of manually sense-annotated examples, to completely unsupervised methods~\cite{yarowsky1995unsupervised} that cluster occurrences of words, thereby inducing word senses.

Supervised learning approaches have been remarkably successful for performing word sense disambiguation. But the lack of sense-tagged data poses a severe bottleneck. To address this problem, researchers have resorted to semi-supervised learning algorithms. The Yarowsky algorithm~\cite{yarowsky1995unsupervised} was an early example of such an algorithm. It uses the `One sense per collocation' and the `One sense per discourse' properties of human languages for word sense disambiguation. From observation, words tend to exhibit only one sense in most given discourse and in a given collocation. The common motif in approaches like ~\cite{yarowsky1995unsupervised},~\cite{le2008semi} is that they allows both labeled and unlabeled data. Training involve iteratively labeling the unlabeled data using an initial seeding labeled set. The seed labeled data is used to learn a classifier which is used to assign labels to unlabeled data, which is merged with the initial seed data to obtain a larger dataset for the next iteration. Finally, the classifier is learned from the extended labelled dataset.
More recently, researchers ~\cite{Ng} have leveraged word alignment information from parallel corporas to aid in obtaining coarse grained senses. The aim of word-alignment task is to align a word in a given sentence to its translated counterpart in another sentence in a different language. Unlike sense-tagged datasets, good quality parallel corpora are readily available . 

%The bootstrapping approach starts from a small amount of seed data for each word: either manually tagged training examples or a small number of surefire decision rules (e.g., `play' in the context of `bass' almost always indicates the musical instrument). The seeds are used to train an initial classifier, using any supervised method. This classifier is then used on the untagged portion of the corpus to extract a larger training set, in which only the most confident classifications are included. The process repeats, each new classifier being trained on a successively larger training corpus, until the whole corpus is consumed, or until a given maximum number of iterations is reached.

Other semi-supervised techniques use large quantities of untagged corpora to provide co-occurrence information that supplements the tagged corpora. These techniques have the potential to help in the adaptation of supervised models to different domains. The reader is encouraged to peruse a more general  survey provided in~\cite{navigli2009word}. 

\section{Task and Data}
\label{sec:taskAndData}
\instructions{Now describe the task and data in more detail.}

\subsection{The Task}
\label{sec:task}
%\instructions{Now, try to formalize your task as a classification/ranking/... problem. Introduce mathematical/formal notation as necessary. How do you evaluate models, or measure success?}
\begin{figure}[!htbp]
\centering
\includegraphics[scale=0.8]{wsd.png}
\caption{An example of word sense disambiguation system. Image courtesy ~\protect\cite{navilgiblog}}
\end{figure}
We are given a a document $D$ and sequence of target words $(w_1,w_2 \cdots w_n)$ in the document. Each word $w_i$ admits a set of candidate senses $S_i=(s_{i1},s_{i2} \cdots s_{in_i})$ where $|S_i|= n_i$. The task is to assign each word $w_i$ the most appropriate sense from its context in $D$. 
\subsection{The Data}
\label{sec:data}
\instructions{Describe the data you use to train and evaluate your models. Describe where you got it from (include references/citations to published works, or URLs!). Describe and give examples for the features that you have access to.} 
We use datasets from \textbf{SemEval-2007} ``http://www.senseval.org". We choose a specific dataset from ``Task \# 7: Coarse-grained English all-words (Coarse AW)" to do both the training and testing. It is an ongoing series of evaluations of computational semantic analysis systems; it evolved from the Senseval word sense evaluation series. In the chosen dataset, it tags approximately 6,000 words of five running texts with coarse senses. Coarse senses will be based on a clustering of the WordNet sense inventory obtained via a mapping to the Oxford Dictionary of English (ODE), a long-established dictionary which encodes coarse sense distinctions. The dataset contains the coarse-grained sense inventory which is prepared semi-automatically: starting from an automatic clustering of senses produced by Navigli (2006) with the Structural Semantic Interconnections (SSI) algorithm, and then manually validate the clustering for the words occurring in the text. For each content word the dataset provides lemma and part of speech.

Our work also requires some supplemental materials such as ``Wordnet" database and ``WS4J" library. We use the the latest released version for linux-Wordnet 3.0 from http://wordnet.princeton.edu/wordnet/ download/current-version/. We compute the similarity metrics by utilizing WS4J library from https://code. google.com/p/ws4j/.

\section{The Models}
\label{sec:models}

\subsection{Baseline Models}
\label{sec:baseline-models}
\instructions{In order to know how difficult the task is and how well we are doing, we need to know how well a suitable baseline model would perform. Define a baseline model for your task. This may not necessarily be a learned model.}
A naive baseline for word sense disambiguation will be to output the most frequent sense of the word. On coarse grained senses, using this baseline gives an accuracy ranging from 50-60\%.

\subsection{Existing Models}
\label{sec:existing-models}
\instructions{If people have worked on this task before, summarize (and cite) some of the existing models} 
It-makes-sense
\subsection{Proposed Model(s)}
\label{sec:proposed-models}
\instructions{Your models and your procedure for learning them go here. Describe both in detail, even if the learning procedure is standard.}
We employ a graph-based approach to disambiguate senses of a word. Our method is similar to ~\cite{Sinha}. We first create a weighted graph of label dependencies for all candidate senses. For every candidate sense $s_{ij}$ for word $w_i$, we have a node in the graph. This node has edges to the all candidate senses of words within a pre-defined window around word $w_i$. These edges are weighed by the dependency score for the pair of senses. The dependency score captures the relationship between two candidate senses for nearby words. Based on the edge weights we assign scores to each node based on a graph-based measure of centrality. The score of a node denotes the "importance" of the node in the graph, taking into account its relationship with neighboring nodes.

\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Hypotheses}
\label{sec:exper-hypoth}
\instructions{Summarize the hypotheses (research questions) your experiments are designed to test (address). (Note that some of these hypotheses may emerge as you keep working on a problem; you will not necessarily have come up with all the questions you wish to address before you have started building a models for the specific task.}

\subsection{Experimental setup}
\label{sec:experimental-setup}
\instructions{Define test/training/dev data splits, describe how you tuned performance. Describe and your evaluation metric, and define it mathematically.
List the models you will evaluate. Cite any existing tools or software you use to perform your experiments; describe what you implemented yourself. Describe how you obtained the features used by each of the models.}
mention maven

\subsection{Experimental results}
\label{sec:experimental-results}
\instructions{Now give the actual experimental results (use figures/tables/graphs as appropriate), and discuss whether they verify or falsify your hypotheses. How important are the various features your models use (consider ablation studies). How robust are your results? (Look at learning curves, or the variance when you perform cross-validation). Can you perform an error analysis?}

\section{Conclusion}
\instructions{Summarize your findings, and discuss their implications, e.g. for future work, or for related tasks. Discuss also the shortcomings of your proposed approach. }. 

\section*{Your current to-do list}
\paragraph{Done}
\begin{enumerate}
\item Implemented the code.
\item Obtained the Semeval 2007 dataset.
\item Added relevant related work.
\end{enumerate}
\paragraph{Left to do}
\begin{enumerate}
\item Running experiments with different similarity metrics.
\item Write-up.
\end{enumerate}

\bibliographystyle{plain}
\bibliography{CS446projCheckIn}  
\end{document}
